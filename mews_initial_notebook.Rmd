---
title: "R Notebook"
output: html_notebook
---

## Part 1: Statistical Analysis
For the first part of this project, you'll mimic the methodology of the working paper ["Does Federally Subsidized Rental Housing Depress Neighborhood Property Values?"](https://furmancenter.org/research/publication/does-federally-subsidized-rental-housing-depress-neighborhood-property), building a statistical model to explore the effect on sales price of a home being within a half mile of an affordable housing development.

1. Using the sf library, find the closest development to each home.

```{r}
library(dplyr)

library(tidyverse)

library(sf)

```

```{r}
#bringing the data in
sales  <-  read.csv('data/filtered_sales.csv')

details  <-  read.csv('data/property_details.csv')

lihtc  <-  read.csv('data/LIHTC_updated.csv')

barnes  <-  read.csv('data/barnes.csv')
```

```{r}
#removing duplicate rows from the sales table
sales <- sales %>% filter(!duplicated(.)) #distinct is another method to consider here

#creating a sale year column
sales <- sales %>% 
          mutate(sale_yr = str_extract(ownerdate, "([0-9]{4})"),
                 sale_yr = as.integer(sale_yr))

glimpse(sales)
```


```{r}
#separate out the lat and lon from the centroid column in the details df
sale_det <- left_join(sales, details, on = apn) %>%
  mutate(centroid = str_remove_all(centroid, '[()]')) %>% 
  separate(centroid, into = c('lon', 'lat'), sep = ',')  

#turning the sale_det df into an sf object              
sale_det_sf <- st_as_sf(sale_det, coords = c("lon", "lat"), remove = FALSE) %>% 
                  mutate(lat = as.numeric(lat),
                         lon = as.numeric(lon))
glimpse(sale_det_sf)

```

```{r}
#simplifying the barnes and lihtc tables to allow combining
barnes_r = barnes %>% 
            transmute(
              li_id = str_c("TN", str_replace_all(Affordability.Term.Start.Date, "-", ""), sep = ""),
              li_addr = Street.Address,
              li_city = City,
              li_zip = Zip.Code,
              li_start_date = Affordability.Term.Start.Date, 
              li_total_units = Total.units, 
              li_units = Barnes.funded.units,
              li_lat = lat,
              li_lng = lng) %>% 
            mutate(li_origin = "barnes",
                   li_addr = str_to_upper(li_addr))

lihtc_r = lihtc %>% 
            transmute(
              li_id = HUD_ID,
              li_addr = PROJ_ADD, 
              li_city = PROJ_CTY,
              li_zip = PROJ_ZIP,
              li_start_date = YR_PIS, 
              li_total_units = N_UNITSR, 
              li_units = LI_UNITR,
              li_lat = LATITUDE,
              li_lng = LONGITUDE) %>% 
            mutate(li_origin = "lihtc") %>% 
            filter(li_start_date != 8888)%>% #removing developments not confirmed as in service
            transform(li_start_date = as.character(li_start_date))

lihtc_r$li_start_date <- str_replace(lihtc_r$li_start_date, "9999", "2015") #replacing the instance when there is a 9999 in this column with the year noted in the YR_ALLOC column
```

```{r}
#putting the 2 low income dataframes together
li_dev = bind_rows(barnes_r, lihtc_r) %>% 
          mutate(li_start_yr = str_extract(li_start_date, "([0-9]{4})"),
                 li_start_yr = as.integer(li_start_yr))
glimpse(li_dev)
```


```{r}
#checking out properties with duplicate lat and lon values
dup_ll <- li_dev %>% 
  select(li_lat, li_lng) %>% 
  filter(duplicated(.))

li_dev %>% filter(li_lat %in% dup_ll$li_lat & li_lng %in% dup_ll$li_lng) %>% arrange(desc(li_lat))
```
After investigating all of the above, these are multiple units in the same location, not actual duplicates


```{r}
dup_addr = li_dev %>% select(li_addr) %>% filter(duplicated(.)) 

li_dev %>% filter(li_addr %in% dup_addr$li_addr) %>% arrange(li_addr)
```
After some research, it appears the date of 2019 is the accurate date for the Old Hickory development, so I will drop the barnes entry for it.
```{r}
li_dev <- li_dev %>% 
            filter(!(li_addr == "5636 OLD HICKORY BLVD" & li_origin == "barnes"))
```

This data frame is now in good shape to convert to an sf object
```{r}
li_dev_sf <- st_as_sf(li_dev, coords = c("li_lng", "li_lat"), remove=FALSE)
glimpse(li_dev_sf)
```

Now, find the low-income development nearest to each home that was sold
```{r}
#spatial join using the nearest function as the join type
sales_li <- st_join(sale_det_sf, li_dev_sf, join = st_nearest_feature)
glimpse(sales_li)
```

2. Calculate the distance from each home its closest development.
```{r}
#after some research, the distVincentyEllipsoid function is considered the most accurate, so bringing in the relavent library to find the distances
library(geosphere)

sales_li$dist <- distVincentyEllipsoid(cbind(sales_li$lon, sales_li$lat), 
                                       cbind(sales_li$li_lng, sales_li$li_lat))/1609.344 #converting the distance from meters to miles
```

```{r}
#Additional filters we decided on: sale amount under $9000000 and square footage under 9000
sales_li <- sales_li %>% 
  mutate(Tpost = sale_yr - li_start_yr,
         age = sale_yr - year_built,
         tract = as.character(tract)) %>% 
  filter(amount < 9000000,
         square_footage < 9000)


```

```{r}
#saveRDS(sales_li, file = "data/all_sale_li.rds")
```


3. Filter the homes down to those that are within one mile of an affordable housing development.
```{r}
nearli_sales <- sales_li %>% 
                  filter(dist <= 1)
glimpse(nearli_sales)
```

4. For each remaining home, calculate a new column called "group", which is defined according to the following rules. Hint: Use the `case_when` function to do this.  
	* "pre" - for homes where the distance is less than half a mile and whose sale date was 2-5 years prior to the year the development was placed in service  
	* "mid" - for homes where the distance is less than half a mile and whose sale date was 0-2 years prior to the year the development was placed in service  
	* "post" - for homes where the distance is less than half a mile and whose sale date was after the year the development was placed in service   
	* "outside" - for homes where the distance is more than half a mile and whose sale date was no more than 5 years prior to the year the development was placed in service   
	* "other" - All other rows 
```{r}
nearli_sales <- nearli_sales %>% 
                  mutate(group = case_when(dist < 0.5 & sale_yr >= (li_start_yr-5) & sale_yr <= (li_start_yr-2) ~ "pre",
                                           dist < 0.5 & sale_yr > (li_start_yr-2) & sale_yr <= li_start_yr ~ "mid",
                                           dist < 0.5 & sale_yr > li_start_yr ~ "post",
                                           dist >= 0.5 & sale_yr >= (li_start_yr-5) ~ "outside",
                                           TRUE ~ "other"))
nearli_sales %>% 
  group_by(group) %>% 
  count()
```

5. Filter out all rows whose group is "other".
```{r}
nearli_sales <- nearli_sales %>% 
                  filter(group != "other")
```
6. Add an id column containing the id value for the development. - already in there

7. Create a column "Tpost" that, for homes in the "post" group gives the number of years that the sale occurred after the housing development was placed in service.
8. Create a column named "age" which gives the age of the home at the time of sale.
9. Filter down to only sales that took place within the five years before or after the associated development was placed in service.
```{r}
nearli_5yr_sales <- nearli_sales %>% 
                      mutate(group = factor(group), 
                             tract = factor(tract)) %>% 
                      filter(Tpost <= 5 & Tpost >= -5)
glimpse(nearli_5yr_sales)
```

Then build a linear model with target variable the sales amount using the following features:
	- square_footage
	- age of home at time of sale
	- group
	- year 
	- tract
```{r}
reg1 <- lm(amount ~ square_footage + age + relevel(group, ref = 'outside') + sale_yr + tract,
           data = nearli_5yr_sales)
anova(reg1)
summary(reg1)
```
Interpreting these coefficients:
sale amount = -1.353e+07 + 55.4(square footage) - 611.2(house age) - (value associated w/ relevant group) + 7252(sale_yr)                 - (value associated w/ relevant tract)

10. Now, try a model with target being the log of the sale price.
	- square_footage
	- age of home at time of sale
	- group
	- year
	- tract
```{r}
reg2 <- lm(log(amount) ~ square_footage + age + relevel(group, ref = 'outside') + sale_yr + tract,
              data = nearli_5yr_sales)
anova(reg2)
summary(reg2)
```
Interpreting these coefficients:
log(sale amount) = -77.21 + 0.0002672(square footage) - 0.001800(house age) - (value associated w/ relevant group) 
                  + 0.04434(sale_yr) + (value associated w/ relevant tract*)
*unlike in reg1, some of these values are positive, others are negative


11. Continue to explore the data to see if you can improve the models you have.
I messed around with adding/removing/substituting a variety of variables. I found the combination below to yeild the highest Adjusted R-squared value. I will note that removing group does not drop the value by much (.0006) and the same with age (.0023)

```{r}

reg3 <- lm(log(amount) ~ square_footage + age + sale_yr + relevel(group, ref = 'outside') + tract + li_id,
              data = nearli_5yr_sales)

anova(reg3)
summary(reg3)
```

```{r}
nearli_5yr_sales %>% 
  mutate(predicted = predict(reg3, newdata = nearli_5yr_sales)) %>%
  ggplot(aes(x = log(amount), y = predicted)) +
    geom_point(na.rm = TRUE) +
    scale_x_continuous(limits = c(6, 18)) +
    scale_y_continuous(limits = c(6, 18))
```
```{r}
library(scales)
nearli_5yr_sales <- nearli_5yr_sales %>% 
  mutate(predicted_amount = exp(predict(reg3, newdata = nearli_5yr_sales)),
         dif_pred_actual = predicted_amount - amount,
         amount_dol = dollar(amount),
         predicted_amount_dol = dollar(predicted_amount),
         dif_pred_actual_dol = dollar(dif_pred_actual))
```
```{r}
#saveRDS(nearli_5yr_sales, file = "data/nearli_5yr_sales.rds")
```


```{r}
#saveRDS(nearli_sales, file = "data/nearli_sales.rds")
```

```{r}
li_addr_list <- nearli_5yr_sales %>% 
  distinct(li_addr) %>% 
  pull(li_addr)

li_dev_map <- li_dev_sf %>%
  filter(li_addr %in% li_addr_list) 
```

```{r}
#saveRDS(li_dev_map, file = "data/li_dev_map.rds")
```


